# ðŸš€ Fraud Detection MLOps Pipeline â€“ Full Assignment Implementation

**Author:** Yuvraj
**Environment:** Google Cloud Platform (GCP) â€“ Jupyter Notebook VM
**Model:** XGBoost (production), fallback pipelines for SHAP/Fairness/Drift
**Dataset:** Credit Card Fraud Detection (transactions.csv)

---

# ðŸ“Œ Overview

This project demonstrates a **complete MLOps workflow** for a real-time fraud detection system deployed on Google Cloud. It implements modern practices spanning the entire ML lifecycle:

âœ” Model Training
âœ” CI/CD with GitHub Actions
âœ” Docker-based Model Serving API
âœ” Deployment on GKE + Autoscaling
âœ” Load Testing & Observability
âœ” Data Poisoning Attack Simulation
âœ” Explainability (SHAP)
âœ” Fairness Analysis
âœ” Concept Drift Detection

Each requirement from the assignment has been **fully satisfied** and documented below.

---

# ðŸ§­ Project Structure

```
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ add_location.py
â”‚   â”œâ”€â”€ train_explain_fair_monitor.py
â”‚   â”œâ”€â”€ poison_data.py
â”‚   â”œâ”€â”€ train_poisoned_models.py
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py (FastAPI app)
â”‚   â”œâ”€â”€ model/model.pkl
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”œâ”€â”€ locustfile.py
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ cd.yml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ v0/
â”‚   â”œâ”€â”€ v1/
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ README.md
```

---

# ðŸ§© Task 1 â€” CI/CD & API Containerization

### âœ… 1. FastAPI Prediction Service

* Implemented `/predict` endpoint that accepts JSON payload with transaction features.
* Returns prediction + probability.
* Model loaded from serialized XGBoost file.

**Key File:** `api/main.py`

---

### âœ… 2. Dockerization

* Dockerfile builds lightweight Python image
* Installs FastAPI + Uvicorn + model dependencies
* Exposes port `8080`

**Key File:** `Dockerfile`

---

### âœ… 3. GitHub Actions CI/CD â†’ Google Artifact Registry

Workflow performs:

1. Trigger on push to `main`
2. Build Docker image
3. Authenticate to Google Cloud via Workload Identity Federation
4. Push to Artifact Registry
5. Post CML report back to GitHub PR

**Key File:** `.github/workflows/cd.yml`

---

# ðŸ§© Task 2 â€” Deployment, Orchestration & Scaling

### âœ… 1. Deploy FastAPI Service on GKE

Deployment object pulls image from Artifact Registry + runs Uvicorn.

**Key File:**

* `kubernetes/deployment.yaml`

---

### âœ… 2. Expose with LoadBalancer

Creates external endpoint accessible via GCP Load Balancer.

**Key File:**

* `kubernetes/service.yaml`

---

### âœ… 3. Autoscaling with HPA

Horizontal Pod Autoscaler automatically scales pods based on CPU usage.

**Key File:**

* `kubernetes/hpa.yaml`

---

### âœ… 4. Load Testing with Locust

Simulates concurrent `/predict` requests.

**Key File:**

* `locust/locustfile.py`

Observations logged:

* Increased RPS â†’ HPA scales from 1 pod â†’ 3 pods
* Latency monitored via Locust UI in real-time

---

### âœ… 5. OpenTelemetry Instrumentation

Added:

* Custom tracing span `"model_predict_time"`
* Tracks latency of `model.predict()`

This allows GKE observability dashboards to show model-level latency.

---

# ðŸ§© Task 3 â€” MLSecurityOps: Data Poisoning Attack Simulation

### âœ… 1. Generate Poisoned Datasets

Created 3 poisoned versions of v0 dataset:

| File                      | % Flipped                  |
| ------------------------- | -------------------------- |
| `poisoned_2_percent.csv`  | 2% of class 0 flipped to 1 |
| `poisoned_8_percent.csv`  | 8% flipped                 |
| `poisoned_20_percent.csv` | 20% flipped                |

**Script:** `scripts/poison_data.py`

---

### âœ… 2. Versioning with DVC + GCS Remote

* Added poisoned datasets
* Tracked using DVC
* Stored remotely on GCS bucket

---

### âœ… 3. MLflow Experiment Tracking

For each poisoning level, trained separate models.

Logged to MLflow:

* `poisoning_level = 2 | 8 | 20`
* F1-scores
* Model artifacts

**Script:** `scripts/train_poisoned_models.py`

Observations:

* Higher poisoning â†’ F1 drops significantly
* Demonstrates vulnerability of fraud model to label corruption

---

# ðŸ§© Task 4 â€” Explainability, Fairness & Monitoring

## 4.1 Introduce Sensitive Attribute

### âœ… Added synthetic `"location"` column

Randomly assigned:

* `Location_A`
* `Location_B`

**Script:** `scripts/add_location.py`

---

## 4.2 Explain Predictions (SHAP)

### âœ… Model retrained with `location` included

### âœ… SHAP beeswarm summary plot generated

File saved as:

```
artifacts/shap_summary.png
```

Logged to MLflow â†’ `explainability/`

---

## 4.3 Fairness Audit

Using Fairlearn:

* Computed `demographic_parity_difference`
* Logged to MLflow

**Result:** Very low bias (â‰ˆ 0.0011)

---

## 4.4 Concept Drift Detection

### Steps performed:

1. Model trained on v0 dataset
2. Predictions generated on v1 dataset
3. Logged F1, precision, recall
4. Drift comparison plot generated:

```
artifacts/drift_comparison.png
```

Logged under: `monitoring/` in MLflow

### Observations:

* F1_v0_val = 0.6667
* F1_v1_full = 0.6667
  â†’ Very low drift between years

---

# ðŸ“Š MLflow Artifacts Summary

| Artifact                     | Purpose                  |
| ---------------------------- | ------------------------ |
| `shap_summary.png`           | Global explainability    |
| `drift_comparison.png`       | Drift monitoring         |
| `fairness.txt`               | Bias metrics summary     |
| `model/`                     | Serialized model         |
| `model_raw/xgb_model.joblib` | Raw model for deployment |

---

# ðŸ›  Technologies Used

### **MLOps**

* DVC (Data versioning)
* MLflow (Experiment tracking)
* GitHub Actions (CI/CD)
* CML (PR reporting)
* Docker + Artifact Registry
* GKE + HPA autoscaling

### **Modeling**

* XGBoost
* SHAP
* Fairlearn

### **Monitoring**

* OpenTelemetry
* Locust Load Testing

---

# ðŸ§¾ Final Notes

This project demonstrates a complete real-world MLOps production pipeline applied to a fraud detection use-case.
All tasks (1â€“4) have been **implemented, tested, logged, and deployed** on Google Cloud.